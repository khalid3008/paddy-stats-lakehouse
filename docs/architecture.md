# Paddy Statistics Lakehouse – Architecture

This document describes the architecture of an end-to-end lakehouse solution built on AWS to process 40+ years of Sri Lankan paddy statistics (Maha/Yala seasons).  
The project demonstrates production-grade data engineering practices including multi-layered data modelling, distributed computation, metadata-driven ETL, and query-optimised storage design.

---

## 1. High-Level Overview

This solution ingests raw Excel/CSV files containing agricultural statistics, standardises and normalises the data using a Lambda-based ETL workflow, applies a structured Silver → Gold transformation pipeline using Athena CTAS, and exposes analytics-ready fact tables for downstream SQL/BI consumption.

Architecture layers:

Bronze (raw) → Silver (standardised parquet) → Silver Clean (normalised) → Gold (facts) → Views (trends)

Key features:
- Automated schema handling and header detection  
- Resilient cleaning logic for inconsistent real-world government datasets  
- Partitioned Parquet optimised for Athena  
- Business-logic transformations (subcategory handling, district reconciliation, weighted yield)  
- Fully serverless, low-ops design  

---

## 2. Platform Components

### 2.1 Amazon S3 – Lakehouse Storage

Logical data zones:

**Bronze:**  
`s3://<bucket>/bronze/paddy_stats/`  
Raw CSV/Excel files.

**Silver:**  
`s3://<bucket>/silver/paddy_stats/metric_name=<metric>/season=<...>/harvest_year=<...>/`  
Partitioned Parquet generated by Lambda.

**Silver Clean:**  
`s3://<bucket>/silver/paddy_stats_clean/`  
Cleaned and normalised partitions generated via Athena CTAS.

**Gold:**  
`s3://<bucket>/gold/paddy_stats/`  
District-level and national-level facts.

S3 serves as the lakehouse storage layer, providing durable, scalable object storage suitable for analytical workloads.

---

### 2.2 AWS Lambda – Standardisation & Silver ETL

Location: `src/lambda/lambda_function.py`  
Runtime: Python (Pandas, boto3, regex)

Responsibilities:
1. Read raw files from S3.  
2. Detect and flatten multi-row headers.  
3. Remove annotation/footer rows.  
4. Identify district column.  
5. Melt wide spreadsheets into long, tidy format.  
6. Extract metadata:  
   - metric_name  
   - season (from filename)  
   - subcategory (Major/Minor/Rainfed/Total)  
   - year_label  
   - harvest_year (handles ranges like `1978/1979`)  
7. Normalise units, clean numeric values.  
8. Write partitioned Parquet to Silver:

`metric_name=<metric>/season=<season>/harvest_year=<year>/part-<hash>.parquet`

The Lambda function is schema-flexible, idempotent, and reliable in handling heterogeneous historical datasets.

---

### 2.3 Athena & Glue – Query + Transformation Layer

- Workgroup: `paddy_wg`  
- Database: `paddy_db`  

Athena is used for:
- Defining external tables over Silver / Clean data  
- Distributed SQL transformations (CTAS)  
- Creating analytical fact models  

#### Silver External Table  
`paddy_db.paddy_stats`  
Partitioned by: metric_name, season, harvest_year.

#### Silver Clean Table  
`paddy_db.paddy_stats_clean`  
Contains:
- Normalised district names  
- Removal of invalid/non-district rows  
- Spelling reconciliation (e.g., Kilinochchi)  
- Consistent uppercase formatting  
- Subcategory normalisation  

Silver Clean is the curated authoritative dataset.

---

## 3. Gold Layer – Dimensional Fact Models

The Gold layer applies business logic, aggregation, and modelling techniques to create BI-ready fact tables.

### 3.1 District-Level Fact  
**Table:** `paddy_db.fact_paddy_district_year_season`  
**Grain:** district × harvest_year × season

Key logic:
- Uses only subcategory = TOTAL to prevent double counting.  
- Excludes blank/null districts and national totals.  
- Combines all available district metrics into a consolidated fact row.

Columns:
- sown_extent_ha  
- harvested_extent_ha  
- production_mt  
- yield_reported_kg_per_ha  
- created_at  

Partitioned by: harvest_year, season.

---

### 3.2 National-Level Fact  
**Table:** `paddy_db.fact_paddy_national_year_season`  
**Grain:** Sri Lanka × harvest_year × season

Logic:
- Aggregates district values using SUM.  
- Computes weighted national yield based on harvested_extent_ha.  

Partitioned by: harvest_year, season.

---

### 3.3 Analytical Trend View  
`paddy_db.v_paddy_national_trends`

Adds:
- YoY % changes in production  
- YoY % changes in yield  
- YoY % changes in harvested extent  

Suitable for BI dashboards and analytical tools.

---

## 4. Data Flow Summary

Bronze → Silver → Silver Clean → Gold → Views

Detailed flow:
1. Raw data uploaded to Bronze.  
2. Lambda performs cleaning and writes Silver Parquet.  
3. Athena CTAS produces Silver Clean.  
4. Gold facts built from curated Silver Clean.  
5. BI-ready trend views exposed for analysis.

---

## 5. Data Quality & Normalisation

### District Rules
- Trim + uppercase  
- Fix spelling variations  
- Remove invalid values (SRI LANKA, HIGHLAND PADDY, DISTRICT header rows)  
- Exclude nulls/blanks  

### Subcategory Rules
- Sources contain Major/Minor/Rainfed/Total  
- Gold uses only TOTAL to avoid duplication  

### Partition Strategy
- Avoid partition explosion  
- Use stable partitions: season, harvest_year  

---

## 6. Design Characteristics

- End-to-end serverless architecture  
- Clean separation of ingestion, standardisation, transformation, and analytics layers  
- Partitioned Parquet for performance and cost efficiency  
- Business-driven fact modelling  
- Robust handling of messy, real-world historical datasets  
- Demonstration of cloud data engineering concepts  

---

## 7. Summary

This project implements an AWS lakehouse architecture with:

- Multi-layer ETL  
- Automated cleaning and metadata extraction  
- Curated Silver and Gold data layers  
- Analytical fact models and trend views  
- Scalable, fully serverless compute/storage design  
